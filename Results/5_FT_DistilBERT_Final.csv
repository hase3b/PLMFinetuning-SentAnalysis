Model,Batch Size,Epochs,Learning Rate,Rank,Alpha,LoRA Dropout,Target Matrices,Accuracy,Precision,Recall,F1-Score
DistilBERT,16,5,0.0001,16,16,0.2,"['attention.q_lin', 'attention.k_lin']",0.91725,0.9174420591316873,0.91725,0.9172339475551818
