{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10328608,"sourceType":"datasetVersion","datasetId":6395305}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### **Installing dependencies**","metadata":{}},{"cell_type":"code","source":"!pip install ipython-autotime gdown evaluate accelerate bitsandbytes peft loralib huggingface_hub transformers peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T00:29:28.298814Z","iopub.execute_input":"2025-01-06T00:29:28.299214Z","iopub.status.idle":"2025-01-06T00:29:37.544677Z","shell.execute_reply.started":"2025-01-06T00:29:28.299178Z","shell.execute_reply":"2025-01-06T00:29:37.543572Z"}},"outputs":[{"name":"stdout","text":"Collecting ipython-autotime\n  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\nCollecting peft\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nCollecting loralib\n  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.5)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.1+cu121)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\nRequirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\nCollecting huggingface_hub\n  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (71.0.4)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\nRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.47)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.18.0)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading loralib-0.1.2-py3-none-any.whl (10 kB)\nDownloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: loralib, huggingface_hub, ipython-autotime, bitsandbytes, peft, evaluate\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.24.7\n    Uninstalling huggingface-hub-0.24.7:\n      Successfully uninstalled huggingface-hub-0.24.7\nSuccessfully installed bitsandbytes-0.45.0 evaluate-0.4.3 huggingface_hub-0.27.0 ipython-autotime-0.3.2 loralib-0.1.2 peft-0.14.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"##### **Importing dependencies**","metadata":{}},{"cell_type":"code","source":"%load_ext autotime\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport os\nimport zipfile\nimport tarfile\nimport re\nimport gdown\nimport gzip\nimport shutil\nimport wandb\nimport time\nimport torch\nimport psutil\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n# import torch_xla.debug.metrics as met\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, precision_recall_fscore_support\nfrom datasets import Dataset\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    AutoModelForCausalLM,\n    AutoModelForSequenceClassification,\n    DistilBertTokenizerFast,\n    DistilBertForSequenceClassification,\n    RobertaTokenizerFast, \n    RobertaForSequenceClassification,\n    GPT2TokenizerFast, \n    GPT2ForSequenceClassification,\n    GenerationConfig,\n    TrainingArguments,\n    Trainer,\n    pipeline,\n    BitsAndBytesConfig,\n    DataCollatorForSeq2Seq,\n    DataCollatorWithPadding,\n    AdamW,\n    get_scheduler\n)\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport time\nimport evaluate\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    TaskType,\n    PeftModel,\n    PeftConfig,\n)\nfrom huggingface_hub import login\nimport kagglehub\n\n# from nltk.corpus import stopwords\n# from nltk import word_tokenize\n# from nltk.stem import WordNetLemmatizer\n# from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n# from sklearn.metrics import accuracy_score\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n# from google.colab import files\n# from scipy.sparse import hstack\n# from gensim.models import Word2Vec\n\nimport warnings\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", message=\".*clean_up_tokenization_spaces.*\")\n# warnings.filterwarnings(\"ignore\", message=\"Some weights of DistilBertForSequenceClassification were not initialized.*\")\nwarnings.filterwarnings(\"ignore\", message=\"Some weights of RobertaForSequenceClassification were not initialized.*\")\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*GradScaler.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*evaluation_strategy.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*gather along dimension 0.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T00:29:57.104806Z","iopub.execute_input":"2025-01-06T00:29:57.105197Z","iopub.status.idle":"2025-01-06T00:30:14.934746Z","shell.execute_reply.started":"2025-01-06T00:29:57.105168Z","shell.execute_reply":"2025-01-06T00:30:14.933883Z"}},"outputs":[{"name":"stdout","text":"time: 17.8 s (started: 2025-01-06 00:29:57 +00:00)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Disable wandb Logging\nos.environ[\"WANDB_MODE\"] = \"disabled\"\nwandb.init()\n\n# Check for GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n# device = xm.xla_device()  # Change device to TPU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T00:30:24.489062Z","iopub.execute_input":"2025-01-06T00:30:24.489745Z","iopub.status.idle":"2025-01-06T00:30:31.395742Z","shell.execute_reply.started":"2025-01-06T00:30:24.489714Z","shell.execute_reply":"2025-01-06T00:30:31.394822Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\ntime: 6.9 s (started: 2025-01-06 00:30:24 +00:00)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"##### **Supporting functions**","metadata":{}},{"cell_type":"code","source":"def clean_review(review):\n    review = re.sub(r'<.*?>', '', review)\n    review = re.sub(r'http\\S+|www\\S+|https\\S+', '', review, flags=re.MULTILINE)\n    review = review.strip()\n    return review\n\ndef preprocess_function(examples):\n    inputs = tokenizer(examples[\"review\"], truncation=True, padding=True, max_length=128)\n    inputs[\"labels\"] = [1 if label.lower() == \"positive\" else 0 for label in examples[\"sentiment\"]]\n    return inputs\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    acc = accuracy_score(labels, preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T00:30:31.396780Z","iopub.execute_input":"2025-01-06T00:30:31.397531Z","iopub.status.idle":"2025-01-06T00:30:31.403685Z","shell.execute_reply.started":"2025-01-06T00:30:31.397498Z","shell.execute_reply":"2025-01-06T00:30:31.402929Z"}},"outputs":[{"name":"stdout","text":"time: 717 µs (started: 2025-01-06 00:30:31 +00:00)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"##### **Loading data**","metadata":{}},{"cell_type":"code","source":"train_df_full = pd.read_csv(\"/kaggle/input/imdb-dataset/train.csv\")\ntrain_df = train_df_full.sample(n=3000, random_state=42)\ntrain_df['review'] = train_df['review'].apply(clean_review)\ntrain_df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T00:30:35.572326Z","iopub.execute_input":"2025-01-06T00:30:35.572644Z","iopub.status.idle":"2025-01-06T00:30:36.521666Z","shell.execute_reply.started":"2025-01-06T00:30:35.572617Z","shell.execute_reply":"2025-01-06T00:30:36.520758Z"}},"outputs":[{"name":"stdout","text":"time: 945 ms (started: 2025-01-06 00:30:35 +00:00)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"test_df_full = pd.read_csv(\"/kaggle/input/imdb-dataset/test.csv\")\ntest_df = test_df_full.sample(n=2000, random_state=42)\ntest_df['review'] = test_df['review'].apply(clean_review)\ntest_df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T00:30:37.729030Z","iopub.execute_input":"2025-01-06T00:30:37.729369Z","iopub.status.idle":"2025-01-06T00:30:38.295039Z","shell.execute_reply.started":"2025-01-06T00:30:37.729340Z","shell.execute_reply":"2025-01-06T00:30:38.294185Z"}},"outputs":[{"name":"stdout","text":"time: 562 ms (started: 2025-01-06 00:30:37 +00:00)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T00:30:45.973299Z","iopub.execute_input":"2025-01-06T00:30:45.973603Z","iopub.status.idle":"2025-01-06T00:30:46.048030Z","shell.execute_reply.started":"2025-01-06T00:30:45.973576Z","shell.execute_reply":"2025-01-06T00:30:46.047177Z"}},"outputs":[{"name":"stdout","text":"time: 70.9 ms (started: 2025-01-06 00:30:45 +00:00)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### **Experimentations for RoBERTa - Phase 1:** keeping LoRA hyperparams fixed","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"roberta-base\"\ntokenizer = RobertaTokenizerFast.from_pretrained(model_checkpoint)\nmodel = RobertaForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2).to(device)\n\ntokenized_train = train_dataset.map(preprocess_function, batched=True)\ntokenized_test = test_dataset.map(preprocess_function, batched=True)\n\n# Fixed LoRA parameters\nrank = 8 \ntarget_matrices = [\"attention.self.query\", \"attention.self.key\", \"attention.self.value\"]\n# target_matrices = [\"attention.self.query\", \"attention.self.key\", \"attention.self.value\", \"attention.output.dense\"]\nlora_alpha = 16\nlora_dropout = 0.1\n\n# Changing hyperparams for batch size, epochs and learning rates\nbatch_sizes = [8, 16]\nepochs_list = [3, 5]\nlearning_rates = [3e-5, 1e-4]\n\ntraining_dropout = 0.1 # Fixed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T23:32:42.932548Z","iopub.execute_input":"2025-01-05T23:32:42.932835Z","iopub.status.idle":"2025-01-05T23:32:54.286247Z","shell.execute_reply.started":"2025-01-05T23:32:42.932813Z","shell.execute_reply":"2025-01-05T23:32:54.285487Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48f34b3bd78440aebc88b25736283c7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fadd45c9ccb4d66b679637570d06482"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"263ad774185146e69f3904ac3e9dc4c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63188561c62646939d0198974a19142c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"843fea4722a847c19a78e60b931d3ddc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"102315b2d2a94399b62d7b92662e4132"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d457d59cf2f94514b1499a1ba63abfa0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc2786d63cb342a6a02f2b78b32b6d33"}},"metadata":{}},{"name":"stdout","text":"time: 11.3 s (started: 2025-01-05 23:32:42 +00:00)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(f\"Model is running on device: {model.device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T23:33:00.778996Z","iopub.execute_input":"2025-01-05T23:33:00.779306Z","iopub.status.idle":"2025-01-05T23:33:00.783785Z","shell.execute_reply.started":"2025-01-05T23:33:00.779278Z","shell.execute_reply":"2025-01-05T23:33:00.783139Z"}},"outputs":[{"name":"stdout","text":"Model is running on device: cuda:0\ntime: 527 µs (started: 2025-01-05 23:33:00 +00:00)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"results_phase_1 = []\n\nfor batch_size in batch_sizes:\n    for epochs in epochs_list:\n        for learning_rate in learning_rates:\n            lora_config = LoraConfig(\n                r=rank,\n                lora_alpha=lora_alpha,\n                target_modules=target_matrices,\n                lora_dropout=lora_dropout,\n                task_type=\"SEQ_CLS\"\n            )\n\n            model_with_lora = get_peft_model(model, lora_config)\n            \n            start_time = time.time()\n            print(f\"\\nRunning experiment with: Batch Size: {batch_size}, Epochs: {epochs}, Learning Rate: {learning_rate}\")\n\n            num_parameters = sum(p.numel() for p in model_with_lora.parameters())\n            trainable_parameters = sum(p.numel() for p in model_with_lora.parameters() if p.requires_grad)\n            trainable_percentage = (trainable_parameters / num_parameters) * 100\n            \n            print(f\"Model has {num_parameters:,} total parameters\")\n            print(f\"Model has {trainable_parameters:,} trainable parameters\")\n            print(f\"{trainable_percentage:.2f}% of the parameters are trainable\")\n\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                gpu_memory = torch.cuda.memory_allocated() / 1024**2  # in MB\n                print(f\"GPU memory allocated: {gpu_memory:.2f} MB\")\n\n            wandb.config.update({\"model/num_parameters\": model.num_parameters()}, allow_val_change=True)\n\n            output_dir = f\"./results_phase1_r{rank}_alpha{lora_alpha}_drop{lora_dropout}_targets{'_'.join(target_matrices)}_bs{batch_size}_epochs{epochs}_lr{learning_rate}\"\n            training_args = TrainingArguments(\n                output_dir=output_dir,\n                evaluation_strategy=\"epoch\",\n                learning_rate=learning_rate,\n                per_device_train_batch_size=8,\n                per_device_eval_batch_size=batch_size,\n                num_train_epochs=epochs,\n                weight_decay=0.01,\n                save_total_limit=1,\n                save_strategy=\"epoch\",\n                logging_dir=\"./logs\",\n                logging_steps=10,\n                load_best_model_at_end=True\n            )\n\n            trainer = Trainer(\n                model=model_with_lora,\n                args=training_args,\n                train_dataset=tokenized_train,\n                eval_dataset=tokenized_test,\n                tokenizer=tokenizer,\n                compute_metrics=compute_metrics\n            )\n\n            trainer.train()\n            metrics = trainer.evaluate()\n\n            end_time = time.time()\n            elapsed_time = end_time - start_time\n            print(f\"Training time: {elapsed_time:.2f} seconds\")\n\n            results_phase_1.append({\n                \"Model\": \"RoBERTa\",\n                \"Batch Size\": batch_size,\n                \"Epochs\": epochs,\n                \"Learning Rate\": learning_rate,\n                \"Rank\": rank,\n                \"Alpha\": lora_alpha,\n                \"LoRA Dropout\": lora_dropout,\n                \"Target Matrices\": target_matrices,\n                \"Accuracy\": metrics[\"eval_accuracy\"],\n                \"Precision\": metrics[\"eval_precision\"],\n                \"Recall\": metrics[\"eval_recall\"],\n                \"F1-Score\": metrics[\"eval_f1\"]                \n            })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T23:33:08.531785Z","iopub.execute_input":"2025-01-05T23:33:08.532066Z","iopub.status.idle":"2025-01-05T23:57:03.457658Z","shell.execute_reply.started":"2025-01-05T23:33:08.532045Z","shell.execute_reply":"2025-01-05T23:57:03.456936Z"}},"outputs":[{"name":"stdout","text":"\nRunning experiment with: Batch Size: 8, Epochs: 3, Learning Rate: 3e-05\nModel has 125,681,668 total parameters\nModel has 1,034,498 trainable parameters\n0.82% of the parameters are trainable\nGPU memory allocated: 480.68 MB\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1125/1125 02:09, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.665900</td>\n      <td>0.650086</td>\n      <td>0.846000</td>\n      <td>0.850790</td>\n      <td>0.846000</td>\n      <td>0.845075</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.295600</td>\n      <td>0.366737</td>\n      <td>0.860000</td>\n      <td>0.861138</td>\n      <td>0.860000</td>\n      <td>0.860032</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.292300</td>\n      <td>0.337767</td>\n      <td>0.874500</td>\n      <td>0.874692</td>\n      <td>0.874500</td>\n      <td>0.874412</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 140.11 seconds\n\nRunning experiment with: Batch Size: 8, Epochs: 3, Learning Rate: 0.0001\nModel has 125,681,668 total parameters\nModel has 1,034,498 trainable parameters\n0.82% of the parameters are trainable\nGPU memory allocated: 508.77 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1125/1125 02:09, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.232400</td>\n      <td>0.308161</td>\n      <td>0.890000</td>\n      <td>0.891022</td>\n      <td>0.890000</td>\n      <td>0.889808</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.235400</td>\n      <td>0.315274</td>\n      <td>0.892500</td>\n      <td>0.892545</td>\n      <td>0.892500</td>\n      <td>0.892511</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.226400</td>\n      <td>0.302369</td>\n      <td>0.893000</td>\n      <td>0.893669</td>\n      <td>0.893000</td>\n      <td>0.892859</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 139.22 seconds\n\nRunning experiment with: Batch Size: 8, Epochs: 5, Learning Rate: 3e-05\nModel has 125,681,668 total parameters\nModel has 1,034,498 trainable parameters\n0.82% of the parameters are trainable\nGPU memory allocated: 508.77 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 03:35, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.541000</td>\n      <td>0.484581</td>\n      <td>0.859500</td>\n      <td>0.861390</td>\n      <td>0.859500</td>\n      <td>0.859090</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.284400</td>\n      <td>0.377795</td>\n      <td>0.861500</td>\n      <td>0.863891</td>\n      <td>0.861500</td>\n      <td>0.861482</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.306900</td>\n      <td>0.309983</td>\n      <td>0.879000</td>\n      <td>0.879258</td>\n      <td>0.879000</td>\n      <td>0.878903</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.398600</td>\n      <td>0.323433</td>\n      <td>0.882500</td>\n      <td>0.883524</td>\n      <td>0.882500</td>\n      <td>0.882290</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.207800</td>\n      <td>0.320414</td>\n      <td>0.882500</td>\n      <td>0.884163</td>\n      <td>0.882500</td>\n      <td>0.882209</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 225.44 seconds\n\nRunning experiment with: Batch Size: 8, Epochs: 5, Learning Rate: 0.0001\nModel has 125,681,668 total parameters\nModel has 1,034,498 trainable parameters\n0.82% of the parameters are trainable\nGPU memory allocated: 508.77 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 03:35, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.237000</td>\n      <td>0.292744</td>\n      <td>0.888500</td>\n      <td>0.890105</td>\n      <td>0.888500</td>\n      <td>0.888235</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.234200</td>\n      <td>0.324362</td>\n      <td>0.895500</td>\n      <td>0.895791</td>\n      <td>0.895500</td>\n      <td>0.895527</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.192700</td>\n      <td>0.304498</td>\n      <td>0.893500</td>\n      <td>0.897061</td>\n      <td>0.893500</td>\n      <td>0.893057</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.326600</td>\n      <td>0.304133</td>\n      <td>0.898000</td>\n      <td>0.897992</td>\n      <td>0.898000</td>\n      <td>0.897993</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.115800</td>\n      <td>0.307258</td>\n      <td>0.899500</td>\n      <td>0.900699</td>\n      <td>0.899500</td>\n      <td>0.899311</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 225.48 seconds\n\nRunning experiment with: Batch Size: 16, Epochs: 3, Learning Rate: 3e-05\nModel has 125,681,668 total parameters\nModel has 1,034,498 trainable parameters\n0.82% of the parameters are trainable\nGPU memory allocated: 508.77 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1125/1125 02:05, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.630600</td>\n      <td>0.606540</td>\n      <td>0.855000</td>\n      <td>0.858986</td>\n      <td>0.855000</td>\n      <td>0.854263</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.292200</td>\n      <td>0.360492</td>\n      <td>0.861000</td>\n      <td>0.861907</td>\n      <td>0.861000</td>\n      <td>0.861037</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.291200</td>\n      <td>0.340752</td>\n      <td>0.876500</td>\n      <td>0.876816</td>\n      <td>0.876500</td>\n      <td>0.876388</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 00:08]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 134.58 seconds\n\nRunning experiment with: Batch Size: 16, Epochs: 3, Learning Rate: 0.0001\nModel has 125,681,668 total parameters\nModel has 1,034,498 trainable parameters\n0.82% of the parameters are trainable\nGPU memory allocated: 508.77 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1125/1125 02:05, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.232400</td>\n      <td>0.308161</td>\n      <td>0.890000</td>\n      <td>0.891022</td>\n      <td>0.890000</td>\n      <td>0.889808</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.235400</td>\n      <td>0.315274</td>\n      <td>0.892500</td>\n      <td>0.892545</td>\n      <td>0.892500</td>\n      <td>0.892511</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.226400</td>\n      <td>0.302369</td>\n      <td>0.893000</td>\n      <td>0.893669</td>\n      <td>0.893000</td>\n      <td>0.892859</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 00:08]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 134.21 seconds\n\nRunning experiment with: Batch Size: 16, Epochs: 5, Learning Rate: 3e-05\nModel has 125,681,668 total parameters\nModel has 1,034,498 trainable parameters\n0.82% of the parameters are trainable\nGPU memory allocated: 508.77 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 03:29, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.541000</td>\n      <td>0.484581</td>\n      <td>0.859500</td>\n      <td>0.861390</td>\n      <td>0.859500</td>\n      <td>0.859090</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.284400</td>\n      <td>0.377795</td>\n      <td>0.861500</td>\n      <td>0.863891</td>\n      <td>0.861500</td>\n      <td>0.861482</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.306900</td>\n      <td>0.309983</td>\n      <td>0.879000</td>\n      <td>0.879258</td>\n      <td>0.879000</td>\n      <td>0.878903</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.398600</td>\n      <td>0.323433</td>\n      <td>0.882500</td>\n      <td>0.883524</td>\n      <td>0.882500</td>\n      <td>0.882290</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.207800</td>\n      <td>0.320414</td>\n      <td>0.882500</td>\n      <td>0.884163</td>\n      <td>0.882500</td>\n      <td>0.882209</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 00:08]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 217.67 seconds\n\nRunning experiment with: Batch Size: 16, Epochs: 5, Learning Rate: 0.0001\nModel has 125,681,668 total parameters\nModel has 1,034,498 trainable parameters\n0.82% of the parameters are trainable\nGPU memory allocated: 508.77 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 03:29, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.237000</td>\n      <td>0.292744</td>\n      <td>0.888500</td>\n      <td>0.890105</td>\n      <td>0.888500</td>\n      <td>0.888235</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.234200</td>\n      <td>0.324362</td>\n      <td>0.895500</td>\n      <td>0.895791</td>\n      <td>0.895500</td>\n      <td>0.895527</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.192700</td>\n      <td>0.304498</td>\n      <td>0.893500</td>\n      <td>0.897061</td>\n      <td>0.893500</td>\n      <td>0.893057</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.326600</td>\n      <td>0.304133</td>\n      <td>0.898000</td>\n      <td>0.897992</td>\n      <td>0.898000</td>\n      <td>0.897993</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.115800</td>\n      <td>0.307257</td>\n      <td>0.899500</td>\n      <td>0.900699</td>\n      <td>0.899500</td>\n      <td>0.899311</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 00:08]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 217.75 seconds\ntime: 23min 54s (started: 2025-01-05 23:33:08 +00:00)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Testing evaluations saved\nresults_df_phase_1 = pd.DataFrame(results_phase_1)\nresults_df_phase_1.to_csv(\"6_FT_RoBERTa_Experiments_FixedLoRA.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T23:57:11.532316Z","iopub.execute_input":"2025-01-05T23:57:11.532657Z","iopub.status.idle":"2025-01-05T23:57:11.541118Z","shell.execute_reply.started":"2025-01-05T23:57:11.532629Z","shell.execute_reply":"2025-01-05T23:57:11.540418Z"}},"outputs":[{"name":"stdout","text":"time: 5.18 ms (started: 2025-01-05 23:57:11 +00:00)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### **Experimentations for RoBERTa - Phase 2:** changing LoRA hyperparameters","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"roberta-base\"\ntokenizer = RobertaTokenizerFast.from_pretrained(model_checkpoint)\nmodel = RobertaForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2).to(device)\n\ntokenized_train = train_dataset.map(preprocess_function, batched=True)\ntokenized_test = test_dataset.map(preprocess_function, batched=True)\n\n# Fixed parameters for batch size and epochs, etc\nfixed_batch_size = 8\nfixed_epochs = 5\nfixed_learning_rate = 1e-4\ntraining_dropout = 0.1\n\n# LoRA parameter combinations\nranks = [8, 16]\ntarget_matrices_list = [\n    [\"attention.self.query\"],\n    [\"attention.self.query\", \"attention.self.key\"],\n    [\"attention.self.query\", \"attention.self.key\", \"attention.self.value\"]\n]\nlora_alpha = 16\nlora_dropouts = [0.1, 0.2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T00:30:59.627106Z","iopub.execute_input":"2025-01-06T00:30:59.627415Z","iopub.status.idle":"2025-01-06T00:31:10.219534Z","shell.execute_reply.started":"2025-01-06T00:30:59.627389Z","shell.execute_reply":"2025-01-06T00:31:10.218812Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6b6e8250c65423cb806c00bf731181b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad0b3e9cc2c546c88dedba03106a5128"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"856be651c75d4e7a920bf889e36eea6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30ccba5679f24437b0e688d4bb1ccd8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d799c4ac29194ed0976010fbaaec3052"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58e384d0d7f1492ebc81fb01beb903cf"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3debc7c10e5d44b6a5e6a4d78fd379c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00e9bc0654834d7cbd08be7a5a438c82"}},"metadata":{}},{"name":"stdout","text":"time: 10.6 s (started: 2025-01-06 00:30:59 +00:00)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"results_phase_2 = []\n\nfor rank in ranks:\n    for target_matrices in target_matrices_list:\n        for lora_dropout in lora_dropouts:\n            lora_config = LoraConfig(\n                r=rank,\n                lora_alpha=lora_alpha,  # Fixed lora_alpha\n                target_modules=target_matrices,\n                lora_dropout=lora_dropout,\n                task_type=\"SEQ_CLS\"\n            )\n\n            model_with_lora = get_peft_model(model, lora_config)\n\n            start_time = time.time()\n            print(f\"\\nRunning experiment with: Rank: {rank}, Target Matrices: {target_matrices}, LoRA Dropout: {lora_dropout}\")\n\n            num_parameters = sum(p.numel() for p in model_with_lora.parameters())\n            trainable_parameters = sum(p.numel() for p in model_with_lora.parameters() if p.requires_grad)\n            trainable_percentage = (trainable_parameters / num_parameters) * 100\n            \n            print(f\"Model has {num_parameters:,} total parameters\")\n            print(f\"Model has {trainable_parameters:,} trainable parameters\")\n            print(f\"{trainable_percentage:.2f}% of the parameters are trainable\")\n\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                gpu_memory = torch.cuda.memory_allocated() / 1024**2  # in MB\n                print(f\"GPU memory allocated: {gpu_memory:.2f} MB\")\n\n            wandb.config.update({\"model/num_parameters\": model.num_parameters()}, allow_val_change=True)\n\n            output_dir = f\"./results_phase2_r{rank}_alpha{lora_alpha}_drop{lora_dropout}_targets{'_'.join(target_matrices)}_bs{fixed_batch_size}_epochs{fixed_epochs}_lr{fixed_learning_rate}\"\n            training_args = TrainingArguments(\n                output_dir=output_dir,\n                evaluation_strategy=\"epoch\",\n                learning_rate=fixed_learning_rate,\n                per_device_train_batch_size=fixed_batch_size,\n                per_device_eval_batch_size=fixed_batch_size,\n                num_train_epochs=fixed_epochs,\n                weight_decay=0.01,\n                save_total_limit=1,\n                save_strategy=\"epoch\",\n                logging_dir=\"./logs\",\n                logging_steps=10,\n                load_best_model_at_end=True\n            )\n\n            trainer = Trainer(\n                model=model_with_lora,\n                args=training_args,\n                train_dataset=tokenized_train,\n                eval_dataset=tokenized_test,\n                tokenizer=tokenizer,\n                compute_metrics=compute_metrics\n            )\n\n            trainer.train()\n            metrics = trainer.evaluate()\n\n            end_time = time.time()\n            elapsed_time = end_time - start_time\n            print(f\"Training time: {elapsed_time:.2f} seconds\")\n\n            results_phase_2.append({\n                \"Model\": \"RoBERTa\",\n                \"Batch Size\": fixed_batch_size,\n                \"Epochs\": fixed_epochs,\n                \"Learning Rate\": fixed_learning_rate,\n                \"Rank\": rank,\n                \"Alpha\": lora_alpha,  # Fixed alpha\n                \"LoRA Dropout\": lora_dropout,\n                \"Target Matrices\": target_matrices,\n                \"Accuracy\": metrics[\"eval_accuracy\"],\n                \"Precision\": metrics[\"eval_precision\"],\n                \"Recall\": metrics[\"eval_recall\"],\n                \"F1-Score\": metrics[\"eval_f1\"]\n            })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T00:31:16.872222Z","iopub.execute_input":"2025-01-06T00:31:16.872537Z","iopub.status.idle":"2025-01-06T01:14:58.857615Z","shell.execute_reply.started":"2025-01-06T00:31:16.872512Z","shell.execute_reply":"2025-01-06T01:14:58.856654Z"}},"outputs":[{"name":"stdout","text":"\nRunning experiment with: Rank: 8, Target Matrices: ['attention.self.query'], LoRA Dropout: 0.1\nModel has 125,386,756 total parameters\nModel has 739,586 trainable parameters\n0.59% of the parameters are trainable\nGPU memory allocated: 479.56 MB\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 03:11, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.323300</td>\n      <td>0.306576</td>\n      <td>0.875500</td>\n      <td>0.875508</td>\n      <td>0.875500</td>\n      <td>0.875467</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.269300</td>\n      <td>0.312752</td>\n      <td>0.885500</td>\n      <td>0.885496</td>\n      <td>0.885500</td>\n      <td>0.885480</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.253600</td>\n      <td>0.303317</td>\n      <td>0.890000</td>\n      <td>0.890477</td>\n      <td>0.890000</td>\n      <td>0.889881</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.479000</td>\n      <td>0.313724</td>\n      <td>0.889500</td>\n      <td>0.890188</td>\n      <td>0.889500</td>\n      <td>0.889350</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.234500</td>\n      <td>0.314326</td>\n      <td>0.891000</td>\n      <td>0.891947</td>\n      <td>0.891000</td>\n      <td>0.890820</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:08]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 202.17 seconds\n\nRunning experiment with: Rank: 8, Target Matrices: ['attention.self.query'], LoRA Dropout: 0.2\nModel has 125,386,756 total parameters\nModel has 739,586 trainable parameters\n0.59% of the parameters are trainable\nGPU memory allocated: 504.27 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 03:12, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.329900</td>\n      <td>0.309357</td>\n      <td>0.871000</td>\n      <td>0.871010</td>\n      <td>0.871000</td>\n      <td>0.871004</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.270800</td>\n      <td>0.317790</td>\n      <td>0.882000</td>\n      <td>0.882010</td>\n      <td>0.882000</td>\n      <td>0.882004</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.252800</td>\n      <td>0.302683</td>\n      <td>0.889500</td>\n      <td>0.889650</td>\n      <td>0.889500</td>\n      <td>0.889436</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.481300</td>\n      <td>0.315852</td>\n      <td>0.889500</td>\n      <td>0.889802</td>\n      <td>0.889500</td>\n      <td>0.889408</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.231200</td>\n      <td>0.316203</td>\n      <td>0.889000</td>\n      <td>0.889654</td>\n      <td>0.889000</td>\n      <td>0.888854</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:08]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 201.62 seconds\n\nRunning experiment with: Rank: 8, Target Matrices: ['attention.self.query', 'attention.self.key'], LoRA Dropout: 0.1\nModel has 125,534,212 total parameters\nModel has 887,042 trainable parameters\n0.71% of the parameters are trainable\nGPU memory allocated: 504.83 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 03:22, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.214600</td>\n      <td>0.324363</td>\n      <td>0.874000</td>\n      <td>0.874064</td>\n      <td>0.874000</td>\n      <td>0.873944</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.330300</td>\n      <td>0.327672</td>\n      <td>0.885500</td>\n      <td>0.885565</td>\n      <td>0.885500</td>\n      <td>0.885453</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.264600</td>\n      <td>0.308685</td>\n      <td>0.889500</td>\n      <td>0.889896</td>\n      <td>0.889500</td>\n      <td>0.889392</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.400900</td>\n      <td>0.312831</td>\n      <td>0.890000</td>\n      <td>0.890059</td>\n      <td>0.890000</td>\n      <td>0.889957</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.234600</td>\n      <td>0.316788</td>\n      <td>0.889500</td>\n      <td>0.890124</td>\n      <td>0.889500</td>\n      <td>0.889359</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 212.01 seconds\n\nRunning experiment with: Rank: 8, Target Matrices: ['attention.self.query', 'attention.self.key'], LoRA Dropout: 0.2\nModel has 125,534,212 total parameters\nModel has 887,042 trainable parameters\n0.71% of the parameters are trainable\nGPU memory allocated: 506.52 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 03:22, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.218700</td>\n      <td>0.332971</td>\n      <td>0.872500</td>\n      <td>0.872497</td>\n      <td>0.872500</td>\n      <td>0.872472</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.343600</td>\n      <td>0.332609</td>\n      <td>0.882000</td>\n      <td>0.882052</td>\n      <td>0.882000</td>\n      <td>0.881954</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.264200</td>\n      <td>0.315015</td>\n      <td>0.889500</td>\n      <td>0.890124</td>\n      <td>0.889500</td>\n      <td>0.889359</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.397100</td>\n      <td>0.314103</td>\n      <td>0.888500</td>\n      <td>0.888592</td>\n      <td>0.888500</td>\n      <td>0.888448</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.240400</td>\n      <td>0.319513</td>\n      <td>0.890000</td>\n      <td>0.890724</td>\n      <td>0.890000</td>\n      <td>0.889846</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:08]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 212.13 seconds\n\nRunning experiment with: Rank: 8, Target Matrices: ['attention.self.query', 'attention.self.key', 'attention.self.value'], LoRA Dropout: 0.1\nModel has 125,681,668 total parameters\nModel has 1,034,498 trainable parameters\n0.82% of the parameters are trainable\nGPU memory allocated: 507.08 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 03:32, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.230900</td>\n      <td>0.312235</td>\n      <td>0.886500</td>\n      <td>0.888753</td>\n      <td>0.886500</td>\n      <td>0.886156</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.248000</td>\n      <td>0.315293</td>\n      <td>0.894500</td>\n      <td>0.895048</td>\n      <td>0.894500</td>\n      <td>0.894530</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.209600</td>\n      <td>0.307568</td>\n      <td>0.890000</td>\n      <td>0.894230</td>\n      <td>0.890000</td>\n      <td>0.889473</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.326400</td>\n      <td>0.308163</td>\n      <td>0.903000</td>\n      <td>0.902995</td>\n      <td>0.903000</td>\n      <td>0.902989</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.155700</td>\n      <td>0.314265</td>\n      <td>0.899500</td>\n      <td>0.900883</td>\n      <td>0.899500</td>\n      <td>0.899292</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 222.66 seconds\n\nRunning experiment with: Rank: 8, Target Matrices: ['attention.self.query', 'attention.self.key', 'attention.self.value'], LoRA Dropout: 0.2\nModel has 125,681,668 total parameters\nModel has 1,034,498 trainable parameters\n0.82% of the parameters are trainable\nGPU memory allocated: 508.77 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 03:33, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.248700</td>\n      <td>0.300775</td>\n      <td>0.886000</td>\n      <td>0.887440</td>\n      <td>0.886000</td>\n      <td>0.885747</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.240200</td>\n      <td>0.318364</td>\n      <td>0.892500</td>\n      <td>0.893109</td>\n      <td>0.892500</td>\n      <td>0.892531</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.209400</td>\n      <td>0.314554</td>\n      <td>0.888500</td>\n      <td>0.892960</td>\n      <td>0.888500</td>\n      <td>0.887942</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.334100</td>\n      <td>0.312220</td>\n      <td>0.902000</td>\n      <td>0.901993</td>\n      <td>0.902000</td>\n      <td>0.901993</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.154100</td>\n      <td>0.309949</td>\n      <td>0.898500</td>\n      <td>0.899607</td>\n      <td>0.898500</td>\n      <td>0.898318</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 223.18 seconds\n\nRunning experiment with: Rank: 16, Target Matrices: ['attention.self.query'], LoRA Dropout: 0.1\nModel has 125,829,124 total parameters\nModel has 1,181,954 trainable parameters\n0.94% of the parameters are trainable\nGPU memory allocated: 508.21 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 03:33, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.179500</td>\n      <td>0.338044</td>\n      <td>0.891500</td>\n      <td>0.893231</td>\n      <td>0.891500</td>\n      <td>0.891231</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.257000</td>\n      <td>0.328147</td>\n      <td>0.895500</td>\n      <td>0.895642</td>\n      <td>0.895500</td>\n      <td>0.895521</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.214400</td>\n      <td>0.327865</td>\n      <td>0.892000</td>\n      <td>0.897144</td>\n      <td>0.892000</td>\n      <td>0.891402</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.311700</td>\n      <td>0.318461</td>\n      <td>0.901500</td>\n      <td>0.901504</td>\n      <td>0.901500</td>\n      <td>0.901502</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.176000</td>\n      <td>0.323620</td>\n      <td>0.898500</td>\n      <td>0.899694</td>\n      <td>0.898500</td>\n      <td>0.898309</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 223.07 seconds\n\nRunning experiment with: Rank: 16, Target Matrices: ['attention.self.query'], LoRA Dropout: 0.2\nModel has 125,829,124 total parameters\nModel has 1,181,954 trainable parameters\n0.94% of the parameters are trainable\nGPU memory allocated: 509.90 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 03:37, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.128100</td>\n      <td>0.451681</td>\n      <td>0.878500</td>\n      <td>0.887510</td>\n      <td>0.878500</td>\n      <td>0.877413</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.234400</td>\n      <td>0.412281</td>\n      <td>0.896000</td>\n      <td>0.896045</td>\n      <td>0.896000</td>\n      <td>0.895965</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.157900</td>\n      <td>0.397570</td>\n      <td>0.890000</td>\n      <td>0.894925</td>\n      <td>0.890000</td>\n      <td>0.889408</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.292900</td>\n      <td>0.369199</td>\n      <td>0.902000</td>\n      <td>0.901994</td>\n      <td>0.902000</td>\n      <td>0.901989</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.139500</td>\n      <td>0.371487</td>\n      <td>0.903000</td>\n      <td>0.903775</td>\n      <td>0.903000</td>\n      <td>0.902865</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 227.74 seconds\n\nRunning experiment with: Rank: 16, Target Matrices: ['attention.self.query', 'attention.self.key'], LoRA Dropout: 0.1\nModel has 125,976,580 total parameters\nModel has 1,329,410 trainable parameters\n1.06% of the parameters are trainable\nGPU memory allocated: 511.02 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 03:37, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.139200</td>\n      <td>0.496340</td>\n      <td>0.870000</td>\n      <td>0.882881</td>\n      <td>0.870000</td>\n      <td>0.868410</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.137500</td>\n      <td>0.435731</td>\n      <td>0.896500</td>\n      <td>0.896555</td>\n      <td>0.896500</td>\n      <td>0.896463</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.157000</td>\n      <td>0.441042</td>\n      <td>0.888500</td>\n      <td>0.894439</td>\n      <td>0.888500</td>\n      <td>0.887803</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.271600</td>\n      <td>0.396598</td>\n      <td>0.898000</td>\n      <td>0.897995</td>\n      <td>0.898000</td>\n      <td>0.897996</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.177400</td>\n      <td>0.413664</td>\n      <td>0.900500</td>\n      <td>0.901534</td>\n      <td>0.900500</td>\n      <td>0.900331</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 227.72 seconds\n\nRunning experiment with: Rank: 16, Target Matrices: ['attention.self.query', 'attention.self.key'], LoRA Dropout: 0.2\nModel has 125,976,580 total parameters\nModel has 1,329,410 trainable parameters\n1.06% of the parameters are trainable\nGPU memory allocated: 512.71 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 03:33, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.138300</td>\n      <td>0.595082</td>\n      <td>0.869000</td>\n      <td>0.883046</td>\n      <td>0.869000</td>\n      <td>0.867275</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.112700</td>\n      <td>0.496073</td>\n      <td>0.897000</td>\n      <td>0.897253</td>\n      <td>0.897000</td>\n      <td>0.896924</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.138700</td>\n      <td>0.458587</td>\n      <td>0.894000</td>\n      <td>0.898815</td>\n      <td>0.894000</td>\n      <td>0.893446</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.318000</td>\n      <td>0.455016</td>\n      <td>0.897500</td>\n      <td>0.897495</td>\n      <td>0.897500</td>\n      <td>0.897487</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.170300</td>\n      <td>0.457460</td>\n      <td>0.897500</td>\n      <td>0.898602</td>\n      <td>0.897500</td>\n      <td>0.897317</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 223.10 seconds\n\nRunning experiment with: Rank: 16, Target Matrices: ['attention.self.query', 'attention.self.key', 'attention.self.value'], LoRA Dropout: 0.1\nModel has 126,124,036 total parameters\nModel has 1,476,866 trainable parameters\n1.17% of the parameters are trainable\nGPU memory allocated: 513.83 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 03:33, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.217500</td>\n      <td>0.320017</td>\n      <td>0.883000</td>\n      <td>0.887625</td>\n      <td>0.883000</td>\n      <td>0.882388</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.235700</td>\n      <td>0.308330</td>\n      <td>0.893500</td>\n      <td>0.893885</td>\n      <td>0.893500</td>\n      <td>0.893529</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.204700</td>\n      <td>0.310269</td>\n      <td>0.892000</td>\n      <td>0.896604</td>\n      <td>0.892000</td>\n      <td>0.891451</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.318400</td>\n      <td>0.298299</td>\n      <td>0.900000</td>\n      <td>0.900069</td>\n      <td>0.900000</td>\n      <td>0.899961</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.145200</td>\n      <td>0.301902</td>\n      <td>0.897000</td>\n      <td>0.898416</td>\n      <td>0.897000</td>\n      <td>0.896782</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 223.38 seconds\n\nRunning experiment with: Rank: 16, Target Matrices: ['attention.self.query', 'attention.self.key', 'attention.self.value'], LoRA Dropout: 0.2\nModel has 126,124,036 total parameters\nModel has 1,476,866 trainable parameters\n1.17% of the parameters are trainable\nGPU memory allocated: 515.52 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 03:32, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.240000</td>\n      <td>0.295665</td>\n      <td>0.886000</td>\n      <td>0.887537</td>\n      <td>0.886000</td>\n      <td>0.885735</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.233400</td>\n      <td>0.309105</td>\n      <td>0.893000</td>\n      <td>0.893519</td>\n      <td>0.893000</td>\n      <td>0.893031</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.200300</td>\n      <td>0.309078</td>\n      <td>0.890500</td>\n      <td>0.895166</td>\n      <td>0.890500</td>\n      <td>0.889935</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.316400</td>\n      <td>0.301800</td>\n      <td>0.903000</td>\n      <td>0.903007</td>\n      <td>0.903000</td>\n      <td>0.902981</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.147800</td>\n      <td>0.303627</td>\n      <td>0.897500</td>\n      <td>0.898778</td>\n      <td>0.897500</td>\n      <td>0.897298</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 222.71 seconds\ntime: 43min 41s (started: 2025-01-06 00:31:16 +00:00)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Testing evaluations saved\nresults_df_phase_2 = pd.DataFrame(results_phase_2)\nresults_df_phase_2.to_csv(\"6_FT_RoBERTa_Experiments_FixedTrainingHyp.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T01:19:28.356467Z","iopub.execute_input":"2025-01-06T01:19:28.356829Z","iopub.status.idle":"2025-01-06T01:19:28.366152Z","shell.execute_reply.started":"2025-01-06T01:19:28.356799Z","shell.execute_reply":"2025-01-06T01:19:28.365267Z"}},"outputs":[{"name":"stdout","text":"time: 5.99 ms (started: 2025-01-06 01:19:28 +00:00)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}