{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10328608,"sourceType":"datasetVersion","datasetId":6395305}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### **Installing dependencies**","metadata":{}},{"cell_type":"code","source":"!pip install ipython-autotime gdown evaluate accelerate bitsandbytes peft loralib huggingface_hub transformers peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T22:41:59.909280Z","iopub.execute_input":"2025-01-05T22:41:59.909709Z","iopub.status.idle":"2025-01-05T22:42:08.622349Z","shell.execute_reply.started":"2025-01-05T22:41:59.909671Z","shell.execute_reply":"2025-01-05T22:42:08.621332Z"}},"outputs":[{"name":"stdout","text":"Collecting ipython-autotime\n  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\nCollecting peft\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nCollecting loralib\n  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.5)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.1+cu121)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\nRequirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\nCollecting huggingface_hub\n  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (71.0.4)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\nRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.47)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.18.0)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading loralib-0.1.2-py3-none-any.whl (10 kB)\nDownloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: loralib, huggingface_hub, ipython-autotime, bitsandbytes, peft, evaluate\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.24.7\n    Uninstalling huggingface-hub-0.24.7:\n      Successfully uninstalled huggingface-hub-0.24.7\nSuccessfully installed bitsandbytes-0.45.0 evaluate-0.4.3 huggingface_hub-0.27.0 ipython-autotime-0.3.2 loralib-0.1.2 peft-0.14.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"##### **Importing dependencies**","metadata":{}},{"cell_type":"code","source":"%load_ext autotime\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport os\nimport zipfile\nimport tarfile\nimport re\nimport gdown\nimport gzip\nimport shutil\nimport wandb\nimport time\nimport torch\nimport psutil\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, precision_recall_fscore_support\nfrom datasets import Dataset\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    AutoModelForCausalLM,\n    AutoModelForSequenceClassification,\n    DistilBertTokenizerFast,\n    DistilBertForSequenceClassification,\n    RobertaTokenizerFast, \n    RobertaForSequenceClassification,\n    GPT2TokenizerFast, \n    GPT2ForSequenceClassification,\n    GenerationConfig,\n    TrainingArguments,\n    Trainer,\n    pipeline,\n    BitsAndBytesConfig,\n    DataCollatorForSeq2Seq,\n    DataCollatorWithPadding,\n    AdamW,\n    get_scheduler\n)\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport time\nimport evaluate\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    TaskType,\n    PeftModel,\n    PeftConfig,\n)\nfrom huggingface_hub import login\nimport kagglehub\n\n# from nltk.corpus import stopwords\n# from nltk import word_tokenize\n# from nltk.stem import WordNetLemmatizer\n# from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n# from sklearn.metrics import accuracy_score\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n# from google.colab import files\n# from scipy.sparse import hstack\n# from gensim.models import Word2Vec\n\nimport warnings\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", message=\".*clean_up_tokenization_spaces.*\")\n# warnings.filterwarnings(\"ignore\", message=\"Some weights of DistilBertForSequenceClassification were not initialized.*\")\nwarnings.filterwarnings(\"ignore\", message=\"Some weights of RobertaForSequenceClassification were not initialized.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*evaluation_strategy.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*gather along dimension 0.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T21:49:44.477939Z","iopub.execute_input":"2025-01-05T21:49:44.478267Z","iopub.status.idle":"2025-01-05T21:50:04.677277Z","shell.execute_reply.started":"2025-01-05T21:49:44.478243Z","shell.execute_reply":"2025-01-05T21:50:04.676438Z"}},"outputs":[{"name":"stdout","text":"time: 20.2 s (started: 2025-01-05 21:49:44 +00:00)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Disable wandb Logging\nos.environ[\"WANDB_MODE\"] = \"disabled\"\nwandb.init()\n\n# Check for GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T21:50:48.969389Z","iopub.execute_input":"2025-01-05T21:50:48.970148Z","iopub.status.idle":"2025-01-05T21:50:54.721996Z","shell.execute_reply.started":"2025-01-05T21:50:48.970118Z","shell.execute_reply":"2025-01-05T21:50:54.721269Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\ntime: 5.75 s (started: 2025-01-05 21:50:48 +00:00)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"##### **Supporting functions**","metadata":{}},{"cell_type":"code","source":"def clean_review(review):\n    review = re.sub(r'<.*?>', '', review)\n    review = re.sub(r'http\\S+|www\\S+|https\\S+', '', review, flags=re.MULTILINE)\n    review = review.strip()\n    return review\n\ndef preprocess_function(examples):\n    inputs = tokenizer(examples[\"review\"], truncation=True, padding=True, max_length=512)\n    inputs[\"labels\"] = [1 if label.lower() == \"positive\" else 0 for label in examples[\"sentiment\"]]\n    return inputs\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    acc = accuracy_score(labels, preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T21:50:54.722974Z","iopub.execute_input":"2025-01-05T21:50:54.723748Z","iopub.status.idle":"2025-01-05T21:50:54.729905Z","shell.execute_reply.started":"2025-01-05T21:50:54.723724Z","shell.execute_reply":"2025-01-05T21:50:54.729165Z"}},"outputs":[{"name":"stdout","text":"time: 715 µs (started: 2025-01-05 21:50:54 +00:00)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"##### **Loading data**","metadata":{}},{"cell_type":"code","source":"train_df_full = pd.read_csv(\"/kaggle/input/imdb-dataset/train.csv\")\ntrain_df = train_df_full.sample(n=3000, random_state=42)\ntrain_df['review'] = train_df['review'].apply(clean_review)\ntrain_df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T21:50:57.635583Z","iopub.execute_input":"2025-01-05T21:50:57.635864Z","iopub.status.idle":"2025-01-05T21:50:58.505563Z","shell.execute_reply.started":"2025-01-05T21:50:57.635843Z","shell.execute_reply":"2025-01-05T21:50:58.504824Z"}},"outputs":[{"name":"stdout","text":"time: 866 ms (started: 2025-01-05 21:50:57 +00:00)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"test_df_full = pd.read_csv(\"/kaggle/input/imdb-dataset/test.csv\")\ntest_df = test_df_full.sample(n=2000, random_state=42)\ntest_df['review'] = test_df['review'].apply(clean_review)\ntest_df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T21:50:58.506501Z","iopub.execute_input":"2025-01-05T21:50:58.506761Z","iopub.status.idle":"2025-01-05T21:50:59.067265Z","shell.execute_reply.started":"2025-01-05T21:50:58.506741Z","shell.execute_reply":"2025-01-05T21:50:59.066315Z"}},"outputs":[{"name":"stdout","text":"time: 557 ms (started: 2025-01-05 21:50:58 +00:00)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T21:50:59.068806Z","iopub.execute_input":"2025-01-05T21:50:59.069048Z","iopub.status.idle":"2025-01-05T21:50:59.142291Z","shell.execute_reply.started":"2025-01-05T21:50:59.069028Z","shell.execute_reply":"2025-01-05T21:50:59.141525Z"}},"outputs":[{"name":"stdout","text":"time: 70 ms (started: 2025-01-05 21:50:59 +00:00)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### **Experimentations for RoBERTa - Phase 1:** keeping LoRA hyperparams fixed","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"roberta-base\"\ntokenizer = RobertaTokenizerFast.from_pretrained(model_checkpoint)\nmodel = RobertaForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2).to(device)\n\ntokenized_train = train_dataset.map(preprocess_function, batched=True)\ntokenized_test = test_dataset.map(preprocess_function, batched=True)\n\n# Fixed LoRA parameters\nrank = 8 \ntarget_matrices = [\"attention.self.query\", \"attention.self.key\", \"attention.self.value\"]\n# target_matrices = [\"attention.self.query\", \"attention.self.key\", \"attention.self.value\", \"attention.output.dense\"]\nlora_alpha = 16\nlora_dropout = 0.1\n\n# Changing hyperparams for batch size, epochs and learning rates\nbatch_sizes = [16, 32]\nepochs_list = [3, 5]\nlearning_rates = [3e-5, 1e-4]\n\ntraining_dropout = 0.1 # Fixed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T21:51:04.364550Z","iopub.execute_input":"2025-01-05T21:51:04.364839Z","iopub.status.idle":"2025-01-05T21:51:12.977827Z","shell.execute_reply.started":"2025-01-05T21:51:04.364818Z","shell.execute_reply":"2025-01-05T21:51:12.977150Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68533e6351684d58a8a24fa2aa25fb0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"995113e0a3754ee082223fa8832fa0f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"843ca96a398c4f7887c39448b7e5d32a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3822b3288c434d66b8063f55489ebffd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eaec1e6c4084485b028ea294b7942d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c99ff0dbbe5b467c939507d4dcec3958"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be6731b9b1a74348945ac92c4501aae2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e8b27dbd4194c758deba4e0fee8e9b2"}},"metadata":{}},{"name":"stdout","text":"time: 8.61 s (started: 2025-01-05 21:51:04 +00:00)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(f\"Model is running on device: {model.device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T21:51:15.488377Z","iopub.execute_input":"2025-01-05T21:51:15.488703Z","iopub.status.idle":"2025-01-05T21:51:15.493441Z","shell.execute_reply.started":"2025-01-05T21:51:15.488676Z","shell.execute_reply":"2025-01-05T21:51:15.492615Z"}},"outputs":[{"name":"stdout","text":"Model is running on device: cuda:0\ntime: 497 µs (started: 2025-01-05 21:51:15 +00:00)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Results storage\nresults_phase_1 = []\n\n# Experimenting with batch size, epochs, and learning rate (keeping LoRA parameters fixed)\nfor batch_size in batch_sizes:\n    for epochs in epochs_list:\n        for learning_rate in learning_rates:\n            # LoRA configuration (fixed, with all matrices)\n            lora_config = LoraConfig(\n                r=rank,\n                lora_alpha=lora_alpha,\n                target_modules=target_matrices,\n                lora_dropout=lora_dropout,\n                task_type=\"SEQ_CLS\"\n            )\n\n            # Apply LoRA to the model\n            model_with_lora = get_peft_model(model, lora_config)\n            \n            start_time = time.time()\n            print(f\"\\nRunning experiment with: Batch Size: {batch_size}, Epochs: {epochs}, Learning Rate: {learning_rate}\")\n\n            num_parameters = sum(p.numel() for p in model_with_lora.parameters())\n            trainable_parameters = sum(p.numel() for p in model_with_lora.parameters() if p.requires_grad)\n            trainable_percentage = (trainable_parameters / num_parameters) * 100\n            \n            print(f\"Model has {num_parameters:,} total parameters\")\n            print(f\"Model has {trainable_parameters:,} trainable parameters\")\n            print(f\"{trainable_percentage:.2f}% of the parameters are trainable\")\n\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                gpu_memory = torch.cuda.memory_allocated() / 1024**2  # in MB\n                print(f\"GPU memory allocated: {gpu_memory:.2f} MB\")\n\n            wandb.config.update({\"model/num_parameters\": model.num_parameters()}, allow_val_change=True)\n\n            # Training arguments\n            output_dir = f\"./results_phase1_r{rank}_alpha{lora_alpha}_drop{lora_dropout}_targets{'_'.join(target_matrices)}_bs{batch_size}_epochs{epochs}_lr{learning_rate}\"\n            training_args = TrainingArguments(\n                output_dir=output_dir,\n                evaluation_strategy=\"epoch\",\n                learning_rate=learning_rate,\n                per_device_train_batch_size=batch_size,\n                per_device_eval_batch_size=batch_size,\n                num_train_epochs=epochs,\n                weight_decay=0.01,\n                save_total_limit=1,\n                save_strategy=\"epoch\",\n                logging_dir=\"./logs\",\n                logging_steps=10,\n                load_best_model_at_end=True,\n            )\n\n            # Trainer\n            trainer = Trainer(\n                model=model_with_lora,\n                args=training_args,\n                train_dataset=tokenized_train,\n                eval_dataset=tokenized_test,\n                tokenizer=tokenizer,\n                compute_metrics=compute_metrics\n            )\n\n            # Train and evaluate\n            trainer.train()\n            metrics = trainer.evaluate()\n\n            end_time = time.time()\n            elapsed_time = end_time - start_time\n            print(f\"Training time: {elapsed_time:.2f} seconds\")\n\n            # Log results for Phase 1\n            results_phase_1.append({\n                \"Model\": \"RoBERTa\",\n                \"Batch Size\": batch_size,\n                \"Epochs\": epochs,\n                \"Learning Rate\": learning_rate,\n                \"Rank\": rank,\n                \"Alpha\": lora_alpha,\n                \"LoRA Dropout\": lora_dropout,\n                \"Target Matrices\": target_matrices,\n                \"Accuracy\": metrics[\"eval_accuracy\"],\n                \"Precision\": metrics[\"eval_precision\"],\n                \"Recall\": metrics[\"eval_recall\"],\n                \"F1-Score\": metrics[\"eval_f1\"]                \n            })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T21:51:17.409760Z","iopub.execute_input":"2025-01-05T21:51:17.410039Z","iopub.status.idle":"2025-01-05T22:40:07.746803Z","shell.execute_reply.started":"2025-01-05T21:51:17.410020Z","shell.execute_reply":"2025-01-05T22:40:07.745445Z"}},"outputs":[{"name":"stdout","text":"\nRunning experiment with: Batch Size: 16, Epochs: 3, Learning Rate: 3e-05\nModel has 125,681,668 total parameters\nModel has 1,034,498 trainable parameters\n0.82% of the parameters are trainable\nGPU memory allocated: 480.68 MB\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 08:28, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.682400</td>\n      <td>0.684038</td>\n      <td>0.819500</td>\n      <td>0.819591</td>\n      <td>0.819500</td>\n      <td>0.819373</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.667600</td>\n      <td>0.659038</td>\n      <td>0.875500</td>\n      <td>0.875492</td>\n      <td>0.875500</td>\n      <td>0.875478</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.620700</td>\n      <td>0.619277</td>\n      <td>0.893000</td>\n      <td>0.894390</td>\n      <td>0.893000</td>\n      <td>0.892773</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:41]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 553.68 seconds\n\nRunning experiment with: Batch Size: 16, Epochs: 3, Learning Rate: 0.0001\nModel has 125,681,668 total parameters\nModel has 1,034,498 trainable parameters\n0.82% of the parameters are trainable\nGPU memory allocated: 508.77 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 08:46, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.337800</td>\n      <td>0.233768</td>\n      <td>0.922500</td>\n      <td>0.923176</td>\n      <td>0.922500</td>\n      <td>0.922522</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.168000</td>\n      <td>0.205651</td>\n      <td>0.931000</td>\n      <td>0.931006</td>\n      <td>0.931000</td>\n      <td>0.931002</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.206700</td>\n      <td>0.194490</td>\n      <td>0.931000</td>\n      <td>0.931020</td>\n      <td>0.931000</td>\n      <td>0.930987</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:40]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 569.66 seconds\n\nRunning experiment with: Batch Size: 16, Epochs: 5, Learning Rate: 3e-05\nModel has 125,681,668 total parameters\nModel has 1,034,498 trainable parameters\n0.82% of the parameters are trainable\nGPU memory allocated: 509.40 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='470' max='470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [470/470 14:19, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.681400</td>\n      <td>0.682247</td>\n      <td>0.830500</td>\n      <td>0.830476</td>\n      <td>0.830500</td>\n      <td>0.830478</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.588600</td>\n      <td>0.524482</td>\n      <td>0.901500</td>\n      <td>0.903306</td>\n      <td>0.901500</td>\n      <td>0.901508</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.249300</td>\n      <td>0.214842</td>\n      <td>0.923500</td>\n      <td>0.923741</td>\n      <td>0.923500</td>\n      <td>0.923519</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.256200</td>\n      <td>0.207943</td>\n      <td>0.923500</td>\n      <td>0.923665</td>\n      <td>0.923500</td>\n      <td>0.923460</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.226900</td>\n      <td>0.206272</td>\n      <td>0.924500</td>\n      <td>0.924557</td>\n      <td>0.924500</td>\n      <td>0.924476</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:40]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 902.16 seconds\n\nRunning experiment with: Batch Size: 16, Epochs: 5, Learning Rate: 0.0001\nModel has 125,681,668 total parameters\nModel has 1,034,498 trainable parameters\n0.82% of the parameters are trainable\nGPU memory allocated: 509.40 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='470' max='470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [470/470 14:19, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.316200</td>\n      <td>0.244090</td>\n      <td>0.911000</td>\n      <td>0.915094</td>\n      <td>0.911000</td>\n      <td>0.910940</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.175000</td>\n      <td>0.200909</td>\n      <td>0.931500</td>\n      <td>0.931578</td>\n      <td>0.931500</td>\n      <td>0.931511</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.201400</td>\n      <td>0.185098</td>\n      <td>0.934000</td>\n      <td>0.934046</td>\n      <td>0.934000</td>\n      <td>0.934008</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.198000</td>\n      <td>0.184860</td>\n      <td>0.932000</td>\n      <td>0.932127</td>\n      <td>0.932000</td>\n      <td>0.931970</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.181700</td>\n      <td>0.190533</td>\n      <td>0.932000</td>\n      <td>0.932321</td>\n      <td>0.932000</td>\n      <td>0.931950</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:40]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training time: 902.71 seconds\n\nRunning experiment with: Batch Size: 32, Epochs: 3, Learning Rate: 3e-05\nModel has 125,681,668 total parameters\nModel has 1,034,498 trainable parameters\n0.82% of the parameters are trainable\nGPU memory allocated: 509.40 MB\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-d770d78d5eb4>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# Train and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1939\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2278\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2279\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3317\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3318\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3320\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/parallel_apply.py\", line 84, in _worker\n    output = module(*input, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 1521, in forward\n    return self.base_model(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 197, in forward\n    return self.model.forward(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 1195, in forward\n    outputs = self.roberta(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 832, in forward\n    encoder_outputs = self.encoder(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 521, in forward\n    layer_outputs = layer_module(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 410, in forward\n    self_attention_outputs = self.attention(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 337, in forward\n    self_outputs = self.self(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 226, in forward\n    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 330.12 MiB is free. Process 5936 has 14.42 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 66.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"],"ename":"OutOfMemoryError","evalue":"Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/parallel_apply.py\", line 84, in _worker\n    output = module(*input, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 1521, in forward\n    return self.base_model(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 197, in forward\n    return self.model.forward(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 1195, in forward\n    outputs = self.roberta(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 832, in forward\n    encoder_outputs = self.encoder(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 521, in forward\n    layer_outputs = layer_module(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 410, in forward\n    self_attention_outputs = self.attention(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 337, in forward\n    self_outputs = self.self(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 226, in forward\n    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 330.12 MiB is free. Process 5936 has 14.42 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 66.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"error"},{"name":"stdout","text":"time: 48min 50s (started: 2025-01-05 21:51:17 +00:00)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Testing evaluations saved\nimport pandas as pd\nresults_df_phase_1 = pd.DataFrame(results_phase_1)\nresults_df_phase_1.to_csv(\"6_FT_RoBERTa_Experiments_FixedLoRA.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Experimentations for RoBERTa - Phase 2:** changing LoRA hyperparameters","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"roberta-base\"\ntokenizer = RobertaTokenizerFast.from_pretrained(model_checkpoint)\nmodel = RobertaForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2).to(device)\n\ntokenized_train = train_dataset.map(preprocess_function, batched=True)\ntokenized_test = test_dataset.map(preprocess_function, batched=True)\n\n# Fixed parameters for batch size and epochs, etc\nfixed_batch_size = 16\nfixed_epochs = 5\nfixed_learning_rate = 1e-4\ntraining_dropout = 0.1\n\n# LoRA parameter combinations\nranks = [8, 16]\ntarget_matrices_list = [\n    [\"attention.self.query\"],\n    [\"attention.self.query\", \"attention.self.key\"],\n    [\"attention.self.query\", \"attention.self.key\", \"attention.self.value\"],\n    [\"attention.self.query\", \"attention.self.key\", \"attention.self.value\", \"attention.output.dense\"]\n]\nlora_alpha = 16\nlora_dropouts = [0.1, 0.2]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Results storage for Phase 2\nresults_phase_2 = []\n\n# Experimenting with LoRA parameters (keeping batch size, epochs, learning rate, and training dropout fixed)\nfor rank in ranks:\n    for target_matrices in target_matrices_list:\n        for lora_dropout in lora_dropouts:\n            # LoRA configuration (varying LoRA parameters)\n            lora_config = LoraConfig(\n                r=rank,\n                lora_alpha=lora_alpha,  # Fixed lora_alpha\n                target_modules=target_matrices,\n                lora_dropout=lora_dropout,\n                task_type=\"SEQ_CLS\"\n            )\n\n            # Apply LoRA to the model\n            model_with_lora = get_peft_model(model, lora_config)\n\n            start_time = time.time()\n            print(f\"\\nRunning experiment with: Rank: {rank}, Target Matrices: {target_matrices}, LoRA Dropout: {lora_dropout}\")\n\n            num_parameters = sum(p.numel() for p in model_with_lora.parameters())\n            trainable_parameters = sum(p.numel() for p in model_with_lora.parameters() if p.requires_grad)\n            trainable_percentage = (trainable_parameters / num_parameters) * 100\n            \n            print(f\"Model has {num_parameters:,} total parameters\")\n            print(f\"Model has {trainable_parameters:,} trainable parameters\")\n            print(f\"{trainable_percentage:.2f}% of the parameters are trainable\")\n\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                gpu_memory = torch.cuda.memory_allocated() / 1024**2  # in MB\n                print(f\"GPU memory allocated: {gpu_memory:.2f} MB\")\n\n            wandb.config.update({\"model/num_parameters\": model.num_parameters()}, allow_val_change=True)\n\n            # Training arguments (fixed batch size, epochs, learning rate, and training dropout)\n            output_dir = f\"./results_phase2_r{rank}_alpha{lora_alpha}_drop{lora_dropout}_targets{'_'.join(target_matrices)}_bs{fixed_batch_size}_epochs{fixed_epochs}_lr{fixed_learning_rate}\"\n            training_args = TrainingArguments(\n                output_dir=output_dir,\n                evaluation_strategy=\"epoch\",\n                learning_rate=fixed_learning_rate,\n                per_device_train_batch_size=fixed_batch_size,\n                per_device_eval_batch_size=fixed_batch_size,\n                num_train_epochs=fixed_epochs,\n                weight_decay=0.01,\n                save_total_limit=1,\n                save_strategy=\"epoch\",\n                logging_dir=\"./logs\",\n                logging_steps=10,\n                load_best_model_at_end=True,\n            )\n\n            # Trainer\n            trainer = Trainer(\n                model=model_with_lora,\n                args=training_args,\n                train_dataset=tokenized_train,\n                eval_dataset=tokenized_test,\n                tokenizer=tokenizer,\n                compute_metrics=compute_metrics\n            )\n\n            # Train and evaluate\n            trainer.train()\n            metrics = trainer.evaluate()\n\n            end_time = time.time()\n            elapsed_time = end_time - start_time\n            print(f\"Training time: {elapsed_time:.2f} seconds\")\n\n            # Log results for Phase 2\n            results_phase_2.append({\n                \"Model\": \"RoBERTa\",\n                \"Batch Size\": fixed_batch_size,\n                \"Epochs\": fixed_epochs,\n                \"Learning Rate\": fixed_learning_rate,\n                \"Rank\": rank,\n                \"Alpha\": lora_alpha,  # Fixed alpha\n                \"LoRA Dropout\": lora_dropout,\n                \"Target Matrices\": target_matrices,\n                \"Accuracy\": metrics[\"eval_accuracy\"],\n                \"Precision\": metrics[\"eval_precision\"],\n                \"Recall\": metrics[\"eval_recall\"],\n                \"F1-Score\": metrics[\"eval_f1\"]\n            })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Testing evaluations saved\nresults_df_phase_2 = pd.DataFrame(results_phase_2)\nresults_df_phase_2.to_csv(\"6_FT_RoBERTa_Experiments_FixedTrainingHyp.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}